{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/   etl.py        README.md               \u001b[01;34mspark-warehouse\u001b[0m/\n",
      "dl.cfg  \u001b[01;34m__pycache__\u001b[0m/  songs_table_image1.PNG  Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "import calendar\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType\n",
    "from pyspark.sql.functions import udf, col, to_timestamp, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Dat, TimestampType\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function scans all the spark components\"\"\"\n",
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data =''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\" Processing log data from the JSON file provided in the S3 bucket\"\"\"  \n",
    "    # get filepath to log data file\n",
    "log_data = input_data + 'data/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data_schema = StructType([\n",
    "        StructField(\"artist\", StringType(), True),\n",
    "        StructField(\"auth\", StringType(), True),\n",
    "        StructField(\"firstName\", StringType(), True),\n",
    "        StructField(\"gender\", StringType(), True),\n",
    "        StructField(\"itemInSession\", LongType(), True),\n",
    "        StructField(\"lastName\", StringType(), True),\n",
    "        StructField(\"length\", DoubleType(), True),\n",
    "        StructField(\"level\", StringType(), True),\n",
    "        StructField(\"location\", StringType(), True),\n",
    "        StructField(\"method\", StringType(), True),\n",
    "        StructField(\"page\", StringType(), True),\n",
    "        StructField(\"registration\", DoubleType(), True),\n",
    "        StructField(\"sessionId\", LongType(), True),\n",
    "        StructField(\"song\", StringType(), True),\n",
    "        StructField(\"status\", LongType(), True),\n",
    "        StructField(\"ts\", LongType(), True),\n",
    "        StructField(\"userAgent\", StringType(), True),\n",
    "        StructField(\"userId\", StringType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "john = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = john.read.json(log_data, schema=log_data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- creating the time table ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- creating the time table ---\")\n",
    "    # create timestamp column from original timestamp column\n",
    "df = df.withColumn(\"start_time\", F.to_timestamp(F.from_unixtime((col(\"ts\") / 1000) , 'yyyy-MM-dd HH:mm:ss.SSS')).cast(\"Timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table = df.select(\"start_time\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         start_time|\n",
      "+-------------------+\n",
      "|2018-11-15 07:49:47|\n",
      "|2018-11-15 19:43:20|\n",
      "|2018-11-21 02:40:02|\n",
      "|2018-11-21 10:52:12|\n",
      "|2018-11-21 13:48:37|\n",
      "|2018-11-21 19:46:29|\n",
      "|2018-11-14 04:37:40|\n",
      "|2018-11-14 12:14:41|\n",
      "|2018-11-14 16:19:02|\n",
      "|2018-11-14 23:03:00|\n",
      "+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+----+---+----+----+-------+\n",
      "|         start_time|month|year|day|hour|week|weekday|\n",
      "+-------------------+-----+----+---+----+----+-------+\n",
      "|2018-11-15 07:49:47|   11|2018| 15|   7|  46|      5|\n",
      "|2018-11-15 19:43:20|   11|2018| 15|  19|  46|      5|\n",
      "|2018-11-21 02:40:02|   11|2018| 21|   2|  47|      4|\n",
      "|2018-11-21 10:52:12|   11|2018| 21|  10|  47|      4|\n",
      "|2018-11-21 13:48:37|   11|2018| 21|  13|  47|      4|\n",
      "+-------------------+-----+----+---+----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table = time_table.withColumn(\"month\", F.month(\"start_time\"))\n",
    "time_table = time_table.withColumn(\"year\", F.year(\"start_time\"))\n",
    "time_table = time_table.withColumn(\"day\", F.dayofmonth(\"start_time\"))\n",
    "time_table = time_table.withColumn(\"hour\", F.hour(\"start_time\"))\n",
    "time_table = time_table.withColumn(\"week\", F.weekofyear(\"start_time\"))\n",
    "time_table = time_table.withColumn(\"weekday\", F.dayofweek(\"start_time\"))\n",
    "\n",
    "time_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
